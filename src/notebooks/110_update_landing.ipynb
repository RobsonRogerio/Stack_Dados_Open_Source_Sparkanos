{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c2b44f8-81cf-4677-a3bf-67fb009b1947",
   "metadata": {},
   "source": [
    "# update_landing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcb9343f-751f-4465-88f1-d7d550fb980c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n",
      "Requirement already satisfied: python-dotenv in /opt/conda/lib/python3.10/site-packages (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccd26d97-60c1-4c0c-9bec-2349c8a84c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "from configs import configs\n",
    "from functions import functions as F\n",
    "from pyspark.sql import functions as func\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import socket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d918d03-8a40-4546-9a5f-6af6094230ff",
   "metadata": {},
   "source": [
    "## Import Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cd55f32-e80f-4b20-be1d-ed2d90467253",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "HOST_ADDRESS=os.getenv('HOST_ADDRESS')\n",
    "MINIO_ACCESS_KEY=os.getenv('MINIO_ACCESS_KEY')\n",
    "MINIO_SECRET_KEY=os.getenv('MINIO_SECRET_KEY')\n",
    "\n",
    "USER_POSTGRES=os.getenv('USER_POSTGRES')\n",
    "PASSWORD_POSTGRES=os.getenv('PASSWORD_POSTGRES')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab74f57-3291-47c0-bea6-c2b1bcef5cea",
   "metadata": {},
   "source": [
    "## Spark Session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3ee348-dbb2-454c-95f6-ab93a243ad2b",
   "metadata": {},
   "source": [
    "A funÃ§Ã£o abaixo foi alterada conforme sugestÃ£o do chat gpt no arquivo 114_update_landing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d3f7bf8-e1eb-40a5-9ef1-6ace462fe088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_spark():\n",
    "    \"\"\"Configure and return a SparkSession.\"\"\"    \n",
    "    \n",
    "    print(\"ðŸ”Ž DEBUG DNS:\")\n",
    "    try:\n",
    "        print(\"Host MinIO resolve para:\", socket.gethostbyname(\"minio\"))\n",
    "    except Exception as e:\n",
    "        print(\"Erro ao resolver MinIO:\", e)\n",
    "\n",
    "    print(\"MINIO_ACCESS_KEY =\", os.getenv(\"MINIO_ACCESS_KEY\"))\n",
    "    print(\"MINIO_SECRET_KEY =\", os.getenv(\"MINIO_SECRET_KEY\"))\n",
    "    print(\"HADOOP_CONF_DIR =\", os.getenv(\"HADOOP_CONF_DIR\"))\n",
    "    \n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"update_landing\") \\\n",
    "        .config(\"spark.master\", \"spark://spark-master:7077\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", f\"http://minio:9000\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", MINIO_ACCESS_KEY) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", MINIO_SECRET_KEY) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", True) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "        .config(\"hive.metastore.uris\", \"thrift://metastore:9083\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1e68e9-e58a-4858-80bf-1b60908a5bb2",
   "metadata": {},
   "source": [
    "## Function Ingest Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5a0e9b8-7c0c-4373-903f-783908eccbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_data():\n",
    "    \n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    \n",
    "    logging.info(\"Starting ingestion...\")\n",
    "    \n",
    "    output_prefix_layer = configs.prefix_layer_name['0']\n",
    "    storage_output = configs.lake_path['landing_adventure_works']\n",
    "    \n",
    "    for key, value in configs.tables_postgres_adventureworks.items():\n",
    "        table = value\n",
    "        table_name = F.convert_table_name(table)\n",
    "        \n",
    "        output_path = f'{storage_output}{output_prefix_layer}{table_name}'\n",
    "        \n",
    "        try:\n",
    "            max_modified_date_destination = spark.read.format(\"parquet\") \\\n",
    "                .load(output_path) \\\n",
    "                .select(func.max(\"modifieddate\") \\\n",
    "                .alias(\"max_modifieddate\")) \\\n",
    "                .collect()[0][\"max_modifieddate\"]\n",
    "            #print(max_modified_date_destination)\n",
    "            \n",
    "            query = f\"\"\" select * from {table} where modifieddate > '{max_modified_date_destination}'\"\"\"\n",
    "            \n",
    "            df_input_data = spark.read \\\n",
    "                .format(\"jdbc\") \\\n",
    "                .option(\"url\", f\"jdbc:postgresql://postgres_adventureworks:5432/Adventureworks\") \\\n",
    "                .option(\"user\", USER_POSTGRES) \\\n",
    "                .option(\"dbtable\", f\"({query}) as filtered\") \\\n",
    "                .option(\"password\", PASSWORD_POSTGRES) \\\n",
    "                .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "                .load()\n",
    "            \n",
    "            input_data_count = df_input_data.count()\n",
    "            logging.info(f\"Number os rows processed for table {table_name}: {input_data_count}\")\n",
    "            \n",
    "            if input_data_count == 0:\n",
    "                logging.info(f\"No new data to process for table {table_name}.\")\n",
    "                continue\n",
    "                      \n",
    "            df_with_update_date = F.add_metadata(df_input_data)\n",
    "            df_with_month_key = F.add_month_key(df_with_update_date, 'modifieddate')\n",
    "            \n",
    "            df_with_month_key.write.format(\"delta\").mode(\"append\").partitionBy(\"month_key\").save(output_path)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing table {table_name}: {str(e)}.\")\n",
    "    \n",
    "    logging.info(\"Ingestion completed!\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b975f7-dcd0-45a7-8407-21cf6ea8ff9b",
   "metadata": {},
   "source": [
    "## Function Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a5c73ef-2951-4223-bbc6-2bdff16d647b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 11:56:05,123 - INFO - Starting ingestion...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”Ž DEBUG DNS:\n",
      "Host MinIO resolve para: 172.21.0.6\n",
      "MINIO_ACCESS_KEY = chapolin\n",
      "MINIO_SECRET_KEY = mudar@123\n",
      "HADOOP_CONF_DIR = None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 11:56:05,765 - INFO - Error while sending or receiving.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 503, in send_command\n",
      "    self.socket.sendall(command.encode(\"utf-8\"))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-06-24 11:56:05,768 - INFO - Closing down clientserver connection\n",
      "2025-06-24 11:56:05,769 - INFO - Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 503, in send_command\n",
      "    self.socket.sendall(command.encode(\"utf-8\"))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 506, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending\n",
      "2025-06-24 11:56:05,774 - INFO - Closing down clientserver connection\n",
      "2025-06-24 11:56:06,076 - INFO - Number os rows processed for table sales_countryregioncurrency: 0\n",
      "2025-06-24 11:56:06,077 - INFO - No new data to process for table sales_countryregioncurrency.\n",
      "2025-06-24 11:56:08,812 - INFO - Number os rows processed for table sales_creditcard: 0\n",
      "2025-06-24 11:56:08,813 - INFO - No new data to process for table sales_creditcard.\n",
      "2025-06-24 11:56:09,359 - INFO - Number os rows processed for table sales_currency: 0\n",
      "2025-06-24 11:56:09,361 - INFO - No new data to process for table sales_currency.\n",
      "2025-06-24 11:56:10,068 - INFO - Number os rows processed for table humanresources_department: 0\n",
      "2025-06-24 11:56:10,069 - INFO - No new data to process for table humanresources_department.\n",
      "2025-06-24 11:56:10,556 - INFO - Number os rows processed for table humanresources_employee: 0\n",
      "2025-06-24 11:56:10,558 - INFO - No new data to process for table humanresources_employee.\n",
      "2025-06-24 11:56:12,160 - INFO - Number os rows processed for table sales_salesorderheader: 0\n",
      "2025-06-24 11:56:12,161 - INFO - No new data to process for table sales_salesorderheader.\n",
      "2025-06-24 11:56:12,163 - INFO - Ingestion completed!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    spark = configure_spark()\n",
    "    ingest_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e36aeba-0b19-4191-a0c1-bed214e0d821",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
